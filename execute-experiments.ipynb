{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Possibilistic Score of OWL Axioms through Support Vector Regression\n",
    "\n",
    "#### Dario Malchiodi, CÃ©lia da Costa Pereira, and Andrea G. B. Tettamanzi\n",
    "\n",
    "## Code to be used in order to reproduce the experiments\n",
    "\n",
    "### First step\n",
    "\n",
    "Execute the cells in this section in order to load the dataset and define the functions to be used in the subsequent experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data format:\n",
    "#\n",
    "# n0, n1, ...\n",
    "# mu0, k00, k01, ..., k0n\n",
    "# ...\n",
    "# mun, kn0, kn1, ..., knn\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "data_file_name = 'data/data-tettamanzi-complete.csv'\n",
    "\n",
    "with open(data_file_name) as data_file:\n",
    "    data = np.array(list(csv.reader(data_file)))\n",
    "\n",
    "n = len(data) - 1\n",
    "\n",
    "names = np.array(data[0])[1:n+1]\n",
    "mu = np.array([float(row[0]) for row in data[1:n+1]])\n",
    "gram = np.array([[float(k.replace('NA', '0')) for k in row[1:n+1]] for row in data[1:n+1]])\n",
    "\n",
    "assert(len(names.shape) == 1)\n",
    "assert(len(mu.shape) == 1)\n",
    "assert(len(gram.shape) == 2)\n",
    "assert(names.shape[0] == gram.shape[0] == gram.shape[1] == mu.shape[0])\n",
    "\n",
    "from possibilearn import *\n",
    "from possibilearn.kernel import PrecomputedKernel\n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "def estimate_possibility(n, mu, cs, gram, num_holdouts, percentages,\n",
    "                         verbose=False, type='eps-svr'):\n",
    "    axiom_indices = range(n)\n",
    "    assert(len(axiom_indices)==len(mu)==n)\n",
    "\n",
    "    paired_axioms = [axiom_indices[i:i+2] for i in range(0, n, 2)]\n",
    "    paired_labels = [mu[i:i+2] for i in range(0, n, 2)]\n",
    "\n",
    "    metrics_membership_rmse = []\n",
    "    metrics_membership_median = []\n",
    "    metrics_membership_stdev = []\n",
    "\n",
    "    metrics_possibility_rmse = []\n",
    "    metrics_possibility_median = []\n",
    "    metrics_possibility_stdev = []\n",
    "\n",
    "    for h in range(num_holdouts):\n",
    "        (paired_values_train,\n",
    "         paired_values_validate,\n",
    "         paired_values_test,\n",
    "         paired_mu_train,\n",
    "         paired_mu_validate,\n",
    "         paired_mu_test) = split_indices(paired_axioms, paired_labels, percentages)\n",
    "\n",
    "        if verbose:\n",
    "            print 'holdout {} of {}'.format(h, num_holdouts)\n",
    "    \n",
    "        out = model_selection_holdout_reg(paired_values_train, paired_mu_train,\n",
    "                                                        paired_values_validate, paired_mu_validate,\n",
    "                                                        cs, gram,\n",
    "                                                        log=True,\n",
    "                                                        verbose=verbose,\n",
    "                                                        type=type)\n",
    "        if type == 'eps-svr':\n",
    "            best_c, best_epsilon, result = out\n",
    "        else:\n",
    "            best_c, result = out\n",
    "        \n",
    "        \n",
    "        if best_c is None:\n",
    "            if verbose:\n",
    "                print 'in holdout {} optimization always failed!'.format(h)\n",
    "            continue\n",
    "    \n",
    "        if verbose:\n",
    "            if type == 'eps-svr':\n",
    "                print 'in holdout {} best epsilon is {}'.format(h, best_epsilon)\n",
    "                \n",
    "            print 'in holdout {} best C is {}'.format(h, best_c)\n",
    "        best_model = result[0]\n",
    "    \n",
    "        # values and labels are still paired, we need to flatten them out\n",
    "        values_train = flatten(paired_values_train)\n",
    "        values_test = flatten(paired_values_test)\n",
    "        mu_test = flatten(paired_mu_test)\n",
    "        \n",
    "        gram_test = [[gram[i, j] for i in values_train] for j in values_test]\n",
    "        \n",
    "        mu_test_hat = best_model.predict(gram_test)\n",
    "        membership_square_err = (mu_test_hat - mu_test)**2\n",
    "        membership_rmse = math.sqrt(sum(membership_square_err) / len(values_test))\n",
    "        metrics_membership_rmse.append(membership_rmse)\n",
    "    \n",
    "        membership_median = np.median(membership_square_err)\n",
    "        metrics_membership_median.append(membership_median)\n",
    "    \n",
    "        membership_stdev = np.std(membership_square_err)\n",
    "        metrics_membership_stdev.append(membership_stdev)\n",
    "        \n",
    "        estimated_mu = mu_test_hat\n",
    "        actual_possibility = [mfi - mnotfi for mfi, mnotfi in zip(mu_test[::2], mu_test[1::2])]\n",
    "        estimated_possibility = [mfi - mnotfi\n",
    "                                 for mfi, mnotfi in zip(estimated_mu[::2], estimated_mu[1::2])]\n",
    "    \n",
    "        possibility_square_err = [(actual - estimated)**2\n",
    "                              for actual, estimated in zip(actual_possibility, estimated_possibility)]\n",
    "        possibility_rmse = math.sqrt(sum(possibility_square_err) / len(possibility_square_err))\n",
    "        metrics_possibility_rmse.append(possibility_rmse)\n",
    "    \n",
    "        possibility_median = np.median(possibility_square_err)\n",
    "        metrics_possibility_median.append(possibility_median)\n",
    "    \n",
    "        possibility_stdev = np.std(possibility_square_err)\n",
    "        metrics_possibility_stdev.append(possibility_stdev)\n",
    "    \n",
    "        indices = ['-'.join(map(str, pair)) for pair in paired_values_test]\n",
    "\n",
    "        results = [(i, phi, notphi, max(phi, notphi), ephi, enotphi, max(ephi, enotphi), p, ep, (p - ep)**2)\n",
    "                   for i, phi, notphi, p, ephi, enotphi, ep in zip(indices, mu_test[::2], mu_test[1::2], actual_possibility,\n",
    "                                                            estimated_mu[::2], estimated_mu[1::2], estimated_possibility)]\n",
    "\n",
    "        results.sort(key = lambda r: r[-1])\n",
    "    \n",
    "        with open('data/axioms-reg-{}-results-holdout-{}-details.csv'.format(type, h), 'w') as output_file:\n",
    "            writer = csv.writer(output_file)\n",
    "            writer.writerows(results)\n",
    "    \n",
    "        with open('data/axioms-reg-{}-results-holdout-{}-global.csv'.format(type, h), 'w') as output_file:\n",
    "            writer = csv.writer(output_file)\n",
    "            writer.writerows([\n",
    "                ('membership RMSE', membership_rmse),\n",
    "                ('membership median', membership_median),\n",
    "                ('membership STDEV', membership_stdev),\n",
    "                ('possibility RMSE', possibility_rmse),\n",
    "                ('possibility median', possibility_median),\n",
    "                ('possibility STDEV', possibility_stdev),\n",
    "            ])\n",
    "        \n",
    "        errors = [r[-1] for r in results]\n",
    "        plt.figure()\n",
    "        p = plt.boxplot(errors)\n",
    "        plt.savefig('data/axioms-reg-{}-results-holdout-{}-boxplot.png'.format(type, h))\n",
    "        plt.clf()\n",
    "    \n",
    "        plt.figure()\n",
    "        p = plt.hist(errors, bins=50)\n",
    "        plt.savefig('data/axioms-reg-{}-results-holdout-{}-histogram.png'.format(type, h))\n",
    "        plt.clf()\n",
    "    \n",
    "        gc.collect()\n",
    "\n",
    "    if verbose:\n",
    "        print 'Membership average values:'\n",
    "        print 'RMSE: {}'.format(np.average(metrics_membership_rmse))\n",
    "        print 'Median: {}'.format(np.average(metrics_membership_median))\n",
    "        print 'STDEV: {}'.format(np.average(metrics_membership_stdev))\n",
    "\n",
    "        print 'Possibility average values:'\n",
    "        print 'RMSE: {}'.format(np.average(metrics_possibility_rmse))\n",
    "        print 'Median: {}'.format(np.average(metrics_possibility_median))\n",
    "        print 'STDEV: {}'.format(np.average(metrics_possibility_stdev))\n",
    "\n",
    "    with open('data/axioms-reg-{}-results-holdout-average-metrics.csv'.format(type), 'w') as output_file:\n",
    "        writer = csv.writer(output_file)\n",
    "        writer.writerows([\n",
    "            ('membership average RMSE', np.average(metrics_membership_rmse)),\n",
    "            ('membership average median', np.average(metrics_membership_median)),\n",
    "            ('membership average STDEV', np.average(metrics_membership_stdev)),\n",
    "            ('possibility average RMSE', np.average(metrics_possibility_rmse)),\n",
    "            ('possibility average median', np.average(metrics_possibility_median)),\n",
    "            ('possibility average STDEV', np.average(metrics_possibility_stdev)),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second step\n",
    "\n",
    "Execute the next cell in order to set up the experiments' parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = PrecomputedKernel(gram)\n",
    "axiom_indices = range(n)\n",
    "\n",
    "cs = (10000, 1000, 100, 10, 1, .1, .01, .001)\n",
    "ks = (k,)\n",
    "\n",
    "num_holdouts = 10\n",
    "percentages = (.8, .1, .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third step: $\\epsilon$-insensitive regression\n",
    "\n",
    "Execute the next cell in order to execute experiments using $\\epsilon$-insensitive regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holdout 0 of 10\n",
      "in holdout 0 best epsilon is 100000\n",
      "in holdout 0 best C is 10000\n",
      "holdout 1 of 10\n",
      "in holdout 1 best epsilon is 100000\n",
      "in holdout 1 best C is 10000\n",
      "holdout 2 of 10\n",
      "in holdout 2 best epsilon is 100000\n",
      "in holdout 2 best C is 10000\n",
      "holdout 3 of 10\n",
      "in holdout 3 best epsilon is 100000\n",
      "in holdout 3 best C is 10000\n",
      "holdout 4 of 10\n",
      "in holdout 4 best epsilon is 100000\n",
      "in holdout 4 best C is 10000\n",
      "holdout 5 of 10\n",
      "in holdout 5 best epsilon is 100000\n",
      "in holdout 5 best C is 10000\n",
      "holdout 6 of 10\n",
      "in holdout 6 best epsilon is 100000\n",
      "in holdout 6 best C is 10000\n",
      "holdout 7 of 10\n",
      "in holdout 7 best epsilon is 100000\n",
      "in holdout 7 best C is 10000\n",
      "holdout 8 of 10\n",
      "in holdout 8 best epsilon is 100000\n",
      "in holdout 8 best C is 10000\n",
      "holdout 9 of 10\n",
      "in holdout 9 best epsilon is 100000\n",
      "in holdout 9 best C is 10000\n",
      "Membership average values:\n",
      "RMSE: 0.48261950076\n",
      "Median: 0.25\n",
      "STDEV: 0.0393892332903\n",
      "Possibility average values:\n",
      "RMSE: 0.845819426046\n",
      "Median: 0.955068081483\n",
      "STDEV: 0.410699535238\n"
     ]
    }
   ],
   "source": [
    "estimate_possibility(n, mu, cs, gram, num_holdouts, percentages, verbose=True, type='eps-svr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth step: ridge regression\n",
    "\n",
    "Execute the next cell in order to execute experiments using ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holdout 0 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malchiodi/Applications/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in holdout 0 best C is 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malchiodi/Applications/anaconda2/lib/python2.7/site-packages/matplotlib/pyplot.py:516: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holdout 1 of 10\n",
      "in holdout 1 best C is 0.001\n",
      "holdout 2 of 10\n",
      "in holdout 2 best C is 10\n",
      "holdout 3 of 10\n",
      "in holdout 3 best C is 0.01\n",
      "holdout 4 of 10\n",
      "in holdout 4 best C is 10000\n",
      "holdout 5 of 10\n",
      "in holdout 5 best C is 0.001\n",
      "holdout 6 of 10\n",
      "in holdout 6 best C is 10\n",
      "holdout 7 of 10\n",
      "in holdout 7 best C is 0.001\n",
      "holdout 8 of 10\n",
      "in holdout 8 best C is 0.01\n",
      "holdout 9 of 10\n",
      "in holdout 9 best C is 0.1\n",
      "Membership average values:\n",
      "RMSE: 0.389328089426\n",
      "Median: 0.0858485703139\n",
      "STDEV: 0.266731187893\n",
      "Possibility average values:\n",
      "RMSE: 0.628384030353\n",
      "Median: 0.190587951058\n",
      "STDEV: 0.637492780973\n"
     ]
    }
   ],
   "source": [
    "estimate_possibility(n, mu, cs, gram, num_holdouts, percentages, verbose=True, type='ridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth step: collect results of Table 2\n",
    "\n",
    "Execute the next cell in order to get the LaTeX code for Table 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_2_header = r'''\\begin{tabular}{ lrrrrrr }\n",
    "& \\multicolumn{3}{c}{Membership} & \\multicolumn{3}{c}{$\\mathrm{ARI}$} \\\\ \n",
    "& RMSE & Median & STDEV & RMSE & Median & STDEV \\\\ \n",
    "\\hline\n",
    "'''\n",
    "\n",
    "table_2_body = r'''$\\epsilon$-sensitive & {eps-mu-avg} & {eps-mu-med} & {eps-mu-std} & {eps-ari-avg} & {eps-ari-med} & {eps-ari-std} \\\\ \n",
    "ridge & {rdg-mu-avg} & {rdg-mu-med} & {rdg-mu-std} & {rdg-ari-avg} & {rdg-ari-med} & {rdg-ari-std} \\\\ \n",
    "'''\n",
    "\n",
    "table_2_footer = r'''\\hline\n",
    "\\end{tabular}'''\n",
    "\n",
    "values_dict = {}\n",
    "\n",
    "names = ('-mu-avg', '-mu-med', '-mu-std', '-ari-avg', '-ari-med', '-ari-std')\n",
    "\n",
    "for kind, header in zip(('eps-svr', 'ridge'), ('eps', 'rdg')):\n",
    "    with open('data/axioms-reg-{}-results-holdout-average-metrics.csv'.format(kind), 'r') as f:\n",
    "        content = f.read()\n",
    "        for name, val in zip(names, content.split('\\r\\n')[:-1]):\n",
    "            #print '{}{}: {:.2e}'.format(header, name, float(val.split(',')[-1]))\n",
    "            values_dict['{}{}'.format(header, name)] = '{:.2e}'.format(float(val.split(',')[-1]))\n",
    "\n",
    "table_2 = table_2_header + '\\n' + table_2_body.format(**values_dict) + '\\n' + table_2_footer\n",
    "print table_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sixth step: get histograms in Fig. 1\n",
    "\n",
    "Histograms in Fig. 1 are picked among those are automatically generated during the third and fourth step and stored in the `data` directory using as name `axioms-reg-<method>-results-holdout-<n>-histogram.png`, where `<method>` is either `ridge` or `eps-svr` and `<n>` is a number between 0 and 9 denoting a given holdout iteration. The directory will also contain boxplots for the same distributions in files having as name `axioms-reg-<method>-results-holdout-<n>-boxplot.png`.\n",
    "\n",
    "## Seventh step: generate Table 3\n",
    "\n",
    "Execute the next cell in order to generate the silhouette values when considering both regression methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "\n",
    "leading_part = 'data/axioms-reg-'\n",
    "middle_part = 'results-holdout-'\n",
    "trailing_part = '-details.csv'\n",
    "\n",
    "fuzzifiers = set()\n",
    "\n",
    "for file_name in glob.glob('data/axioms-reg*details.csv'):\n",
    "    experiment = file_name[len(leading_part):]\n",
    "    sep_position = experiment.find('-results')\n",
    "    fuzzifier = experiment[:sep_position]\n",
    "    fuzzifiers.add(fuzzifier)\n",
    "    \n",
    "table = {}\n",
    "\n",
    "for f in fuzzifiers:\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for file_name in glob.glob('{}{}-*details.csv'.format(leading_part, f)):\n",
    "        experiment = file_name[len(leading_part):]\n",
    "        sep_position= len(f) + len(middle_part)\n",
    "        num_experiment = experiment[sep_position+1:experiment.find(trailing_part)]\n",
    "        with open(file_name, 'r') as csv_file:\n",
    "            data = csv.reader(csv_file)\n",
    "            results = [r for r in data]\n",
    "            df = pd.DataFrame(index=[r[0] for r in results])\n",
    "            df[num_experiment] = [float(r[-1]) for r in results]\n",
    "            dfs.append(df)\n",
    "\n",
    "    result = pd.merge(left=dfs[0], right=dfs[1], how='outer', left_index=True, right_index=True)\n",
    "    for i in range(2, len(dfs)):\n",
    "        result = pd.merge(left=result, right=dfs[i], how='outer', left_index=True, right_index=True)\n",
    "\n",
    "    table[f] = result\n",
    "    table[f]['median'] = table[f].median(numeric_only=True, axis=1)\n",
    "    table[f]['std'] = table[f].std(numeric_only=True, axis=1)\n",
    "\n",
    "avg_dfs = [table[f].filter(['median']) for f in fuzzifiers]\n",
    "fuzzifier_name = [f for f in fuzzifiers]\n",
    "result = pd.merge(left=avg_dfs[0], right=avg_dfs[1], how='outer',\n",
    "                  left_index=True, right_index=True)\n",
    "result.columns = fuzzifier_name[:2]\n",
    "for i in range(2, len(avg_dfs)):\n",
    "    result = pd.merge(left=result, right=avg_dfs[i], how='outer',\n",
    "                      left_index=True, right_index=True)\n",
    "\n",
    "result.columns = fuzzifier_name\n",
    "\n",
    "result_filled = result.copy()\n",
    "\n",
    "m = result_filled.mean(axis=1)\n",
    "for i, col in enumerate(result_filled):\n",
    "    result_filled.iloc[:, i] = result_filled.iloc[:, i].fillna(m)\n",
    "    \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "def cluster_and_score(data, k):\n",
    "    clusterer = KMeans(k)\n",
    "    clusterization = clusterer.fit(data)\n",
    "    return (k, metrics.silhouette_score(result_filled,\n",
    "                                        clusterization.labels_,\n",
    "                                        metric='euclidean'))\n",
    "pd.DataFrame([cluster_and_score(result_filled, k) for k in range(2, 10)],\n",
    "             columns=('$k$', 'Silhouette index'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the next cell in order to generate the silhouette values when considering only $\\epsilon$-insensitive regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_dfs = table['eps-svr'].filter(['median'])\n",
    "eps_dfs.columns = ['eps-svr']\n",
    "\n",
    "result_filled = eps_dfs.copy()\n",
    "\n",
    "m = result_filled.mean(axis=1)\n",
    "for i, col in enumerate(result_filled):\n",
    "    result_filled.iloc[:, i] = result_filled.iloc[:, i].fillna(m)\n",
    "    \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "def cluster_and_score(data, k):\n",
    "    clusterer = KMeans(k)\n",
    "    clusterization = clusterer.fit(data)\n",
    "    return (k, metrics.silhouette_score(result_filled,\n",
    "                                        clusterization.labels_,\n",
    "                                        metric='euclidean'))\n",
    "pd.DataFrame([cluster_and_score(result_filled, k) for k in range(2, 10)],\n",
    "             columns=('$k$', 'Silhouette index'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the next cell in order to generate the silhouette values when considering only ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_dfs = table['ridge'].filter(['median'])\n",
    "ridge_dfs.columns = ['ridge']\n",
    "\n",
    "result_filled = ridge_dfs.copy()\n",
    "\n",
    "m = result_filled.mean(axis=1)\n",
    "for i, col in enumerate(result_filled):\n",
    "    result_filled.iloc[:, i] = result_filled.iloc[:, i].fillna(m)\n",
    "    \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "def cluster_and_score(data, k):\n",
    "    clusterer = KMeans(k)\n",
    "    clusterization = clusterer.fit(data)\n",
    "    return (k, metrics.silhouette_score(result_filled,\n",
    "                                        clusterization.labels_,\n",
    "                                        metric='euclidean'))\n",
    "pd.DataFrame([cluster_and_score(result_filled, k) for k in range(2, 10)],\n",
    "             columns=('$k$', 'Silhouette index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the next cell in order to detect easy and hard axioms for the case of ridge regression, and to save their list in the file `hardness-reg-ridge.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(result_filled)\n",
    "n = 1444 * 2\n",
    "avg = np.array([0., 0.])\n",
    "hardness = [None] * n\n",
    "\n",
    "for p in result_filled.index:\n",
    "    cluster_index = clusterization.predict(result_filled.loc[[p]])[0]\n",
    "    avg[cluster_index] += result_filled.loc[[p]]['ridge'][0]\n",
    "    for i in map(int, p.split('-')):\n",
    "        hardness[i] = cluster_index\n",
    "\n",
    "n_cluster_1 = len([i for i in range(len(hardness)) if hardness[i]])\n",
    "avg /= (float(n-n_cluster_1)/2, float(n_cluster_1)/2)\n",
    "\n",
    "easy_index = avg.argmin()\n",
    "hard_index = 1 - easy_index\n",
    "\n",
    "easy_axioms = [i for i in range(len(hardness)) if hardness[i]==easy_index]\n",
    "hard_axioms = [i for i in range(len(hardness)) if hardness[i]==hard_index]\n",
    "\n",
    "print 'There are {} easy axioms and {} hard axioms.'.format(len(easy_axioms), len(hard_axioms))\n",
    "\n",
    "import json\n",
    "\n",
    "with open('hardness-reg-ridge.json', 'w') as f:\n",
    "    json.dump([1 if hardness[i]==hard_index else 0 for i in range(len(hardness))], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the next cell in order to print indices of all hard axioms detected through ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hardness-reg-ridge.json', 'r') as f:\n",
    "    c = json.load(f)\n",
    "\n",
    "print 'Hard indices for svr-ridge'\n",
    "ridge_hard =  [i for i in range(len(c)) if c[i]==1]\n",
    "print ridge_hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the next cell in order to obtain the indices of axioms in Table 3 and to print the overlap percentage between hard axioms detected with ridge regression and with fuzzy learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hardness.json', 'r') as f:\n",
    "    c = json.load(f)\n",
    "\n",
    "print 'Hard indices for fuzzy-learn'\n",
    "fuzzy_hard = [i for i in range(len(c)) if c[i]==1]\n",
    "print fuzzy_hard\n",
    "\n",
    "print 'common indices between ridge and fuzzy-learn'\n",
    "common = [i for i in ridge_hard if i in fuzzy_hard]\n",
    "print common\n",
    "\n",
    "print 'percentage of overlap'\n",
    "print float(len(common))/len(ridge_hard)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
